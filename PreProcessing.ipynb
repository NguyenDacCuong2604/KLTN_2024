{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NguyenDacCuong2604/KLTN_2024/blob/main/PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOtWeaVBOFaM",
        "outputId": "57e87e80-cebc-4d18-9628-cec0918c6550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Thesis\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd '/content/gdrive/MyDrive/Thesis'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DmqCxou6wHjo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def loaddictchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split('|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split('|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "dicchar = loaddictchar()\n",
        "\n",
        "#Đưa văn bản về bộ mã tiếng Việt utf-8\n",
        "def convertwindown1525toutf8(txt):\n",
        "    return re.sub(r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "                  lambda x: dicchar[x.group()], txt)\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ']]\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i])):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "#Sửa lỗi sử dụng các dấu thanh kiểu cũ (òa, úy thay vì oà, uý)\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "        else:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    sentence = convertwindown1525toutf8(sentence)\n",
        "\n",
        "    ##Do nếu không tách các ký tự đặc biệt ra từng từ, thì def chuan_hoa_dau_tu_tieng_viet sẽ sửa dấu sai\n",
        "    # Sử dụng regex để tách các từ và ký tự đặc biệt\n",
        "    words_with_indices = [(match.group(), match.start()) for match in re.finditer(r'\\w+|[^\\w\\s]', sentence)]\n",
        "\n",
        "    # Chuẩn hóa từng từ tiếng Việt\n",
        "    for index, (word, pos) in enumerate(words_with_indices):\n",
        "        if word.isalnum():  # Chỉ chuẩn hóa nếu từ là chữ cái hoặc số\n",
        "            words_with_indices[index] = (chuan_hoa_dau_tu_tieng_viet(word.lower()), pos)\n",
        "\n",
        "    # Khôi phục lại câu với các ký tự đặc biệt đúng vị trí và giữ nguyên viết hoa viết thường\n",
        "    result = list(sentence)\n",
        "    for word, pos in words_with_indices:\n",
        "        for i, char in enumerate(word):\n",
        "            if sentence[pos + i].isupper():\n",
        "                result[pos + i] = char.upper()\n",
        "            else:\n",
        "                result[pos + i] = char\n",
        "\n",
        "    return ''.join(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hi-qSBoMkQO",
        "outputId": "d8eb2db3-d6f7-4f20-830f-b43ed6cf3817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.5.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.66.4)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.10 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_LFmA7U0M5Wz"
      },
      "outputs": [],
      "source": [
        "# Np - Proper noun\n",
        "# M - Numeral\n",
        "# F - Filtered out (punctuation)\n",
        "def filter_and_join_words(tagged_sentence, is_remove_np):\n",
        "    exclude_pos_tags = ['M', 'F']\n",
        "    if is_remove_np:\n",
        "        exclude_pos_tags.append('Np')\n",
        "\n",
        "    \"\"\"\n",
        "    Remove words that have postags in the exclude_pos_tags list from the tagged_sentence and join the remaining words with spaces.\n",
        "\n",
        "    Parameters:\n",
        "    tagged_sentence (tuple of list of str): The tuple containing two lists: words and postags.\n",
        "    exclude_pos_tags (list of str): The list of postags to exclude.\n",
        "\n",
        "    Returns:\n",
        "    str: The filtered words joined by spaces.\n",
        "    \"\"\"\n",
        "    words, postags = tagged_sentence\n",
        "    filtered_words = []\n",
        "\n",
        "    for word, postag in zip(words, postags):\n",
        "        if postag not in exclude_pos_tags:\n",
        "            filtered_words.append(word)\n",
        "\n",
        "    return ' '.join(filtered_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tqNtn8j6N78O"
      },
      "outputs": [],
      "source": [
        "#Stopwords\n",
        "stopwords = set()\n",
        "#open file stopword\n",
        "with open('/content/gdrive/MyDrive/Thesis/dataset/Stopwords/vietnamese-stopwords-dash.txt', \"r\", encoding='utf-8') as file:\n",
        "  word = file.readlines()\n",
        "stopwords = [n.replace('\\n', '') for n in word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "31obE31bOdpl"
      },
      "outputs": [],
      "source": [
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "def text_preprocess(text, is_remove_np):\n",
        "    text = chuan_hoa_dau_cau_tieng_viet(text) #Chuẩn hóa dấu câu tiếng việt và chuyển đổi Windown1525 thành UTF8\n",
        "    text = text.replace('_x000D_', '') # _x000D_ là mã Unicode biểu diễn ký tự xuống dòng (CR-Carriage Return) (Cái này khi up file excel, mở bằng GG Sheets sẽ bị ở những chổ xuống dòng dữ liệu '\\n')\n",
        "\n",
        "    # Kiểm tra tỷ lệ ký tự viết hoa trong văn bản\n",
        "    upper_case_chars = sum(1 for char in text if char.isupper())\n",
        "    total_chars = len(text)\n",
        "    if upper_case_chars / total_chars >= 0.9:\n",
        "        text = text.lower() #Nếu tỷ lệ ký tự viết hoa cao hơn 90%, chuyển đổi văn bản thành viết thường\n",
        "    \"\"\"\n",
        "    Ví dụ:   V/V CỬ NHÂN SỰ THAM GIA TỔ CÔNG TÁC VÀ TỔ GIÚP VIỆC XÂY DỰNG ĐỀ ÁN PHÂN CẤP, ỦY QUYỀN\n",
        "             BÁO CÁO VỀ TỔ CHỨC HOẠT ĐỘNG VẬN TẢI PHÒNG, CHỐNG DỊCH COVID-19 TRONG TÌNH HÌNH MỚI ĐỐI VỚI LĨNH VỰC GIAO THÔNG VẬN TẢI (Ngày 23 tháng 11 năm 2021)\n",
        "        -Những từ này nếu không đưa về lower thì khi tách từ đa phần sẽ nhận định là danh từ, danh từ riêng,... ra kết quả không chính xác với nội dung của text\n",
        "    \"\"\"\n",
        "\n",
        "    postagging_text = ViPosTagger.postagging(ViTokenizer.tokenize(text)) #Dùng Vi để tách từ và gán postagger\n",
        "    text = filter_and_join_words(postagging_text, is_remove_np) #Loại bỏ các từ có postagger không cần thiết\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) #Loại bỏ các ký tự đặc biệt thêm 1 lần nữa nếu sau khi tách từ vấn còn\n",
        "    word_tokens = text.split()\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stopwords] #Loại bỏ các từ trong bộ stopwords\n",
        "    filtered_text = [word for word in filtered_text if len(word) > 1] #Loại bỏ các từ đơn\n",
        "    filtered_text = ' '.join(filtered_text)\n",
        "    return filtered_text.lower() #Trả về text lower"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tQJJl-aC6iPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_duplicated(df):\n",
        "  # Lấy các hàng trùng lặp dựa trên cột \"Trích yếu\"\n",
        "  duplicated_data = df[df.duplicated(subset='Trích yếu', keep=False)]\n",
        "\n",
        "  grouped = duplicated_data.groupby('Trích yếu')\n",
        "  filtered_data = []\n",
        "  for name, group in grouped:\n",
        "    counts = group['ID phòng xử lý'].value_counts()\n",
        "    if len(counts) == 1:\n",
        "      filtered_data.append(group)\n",
        "    else:\n",
        "      # Lấy số lượng count của counts top 2\n",
        "      top_counts = counts.nlargest(2)\n",
        "      first_top_count = top_counts.iloc[0]\n",
        "      second_top_count = top_counts.iloc[1]\n",
        "      count_data = first_top_count - second_top_count\n",
        "      top_id = counts.idxmax()\n",
        "      top_group = group[group['ID phòng xử lý'] == top_id]\n",
        "      get_data_top = top_group.head(count_data)\n",
        "      filtered_data.append(get_data_top)\n",
        "  filtered_data = pd.concat(filtered_data) #Dữ liệu đã loại bỏ các trích yếu nhiều nhãn\n",
        "\n",
        "  return duplicated_data.drop(filtered_data.index) #Dữ liệu mà trích yếu có nhiều nhãn"
      ],
      "metadata": {
        "id": "X_O_VnfNdIZ4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_input = '/content/gdrive/MyDrive/Thesis/dataset/Root/data_2_nam.csv'\n",
        "keep_data_multi_label = True\n",
        "is_remove_np = True\n",
        "is_k_means = True\n",
        "k = 6\n",
        "count_data = 2000"
      ],
      "metadata": {
        "id": "1FI2H6cxcYwp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PijFiJSS6krX",
        "outputId": "10028fbd-4c83-4405-b520-3590a4e8d95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24618 entries, 0 to 24617\n",
            "Data columns (total 2 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Trích yếu       24618 non-null  object\n",
            " 1   ID phòng xử lý  24618 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 384.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(path_input)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not keep_data_multi_label:\n",
        "  df_duplicated = get_data_duplicated(df)\n",
        "  df = df.drop(df_duplicated.index)\n",
        "df['input'] = df['Trích yếu'].apply(lambda x: text_preprocess(x, is_remove_np))"
      ],
      "metadata": {
        "id": "1fOJhMEQCrkt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BskZwdXh9CGc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e82af6c3-274d-4ff1-86c8-f547f06ae4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 24618 entries, 0 to 24617\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Trích yếu       24618 non-null  object\n",
            " 1   ID phòng xử lý  24618 non-null  int64 \n",
            " 2   input           24618 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 577.1+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['ID phòng xử lý'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfFVEMBWFk1E",
        "outputId": "ee51533b-49b8-485c-e53e-6daacfcf7573"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID phòng xử lý\n",
              "2733546    9396\n",
              "2733552    9068\n",
              "2733540    2601\n",
              "2733570    1593\n",
              "2733522    1345\n",
              "2733534     615\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if is_k_means:\n",
        "  from sklearn.cluster import KMeans\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "  final_df = pd.DataFrame()\n",
        "\n",
        "  for label in df['ID phòng xử lý'].unique():\n",
        "    subset = df[df['ID phòng xử lý'] == label]\n",
        "\n",
        "    if subset.shape[0] < count_data:\n",
        "        final_df = pd.concat([final_df, subset])\n",
        "        continue\n",
        "\n",
        "    ratio = count_data / subset.shape[0]\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X_text = tfidf_vectorizer.fit_transform(subset['input'])\n",
        "    kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "    kmeans.fit(X_text)\n",
        "    labels = kmeans.labels_\n",
        "    subset['Cluster'] = labels\n",
        "\n",
        "    for cluster in range(k):\n",
        "        cluster_data = subset[subset['Cluster'] == cluster]\n",
        "        n_samples = int(ratio * cluster_data.shape[0])\n",
        "        if n_samples > 0:\n",
        "            sampled_data = cluster_data.sample(n=n_samples, random_state=42)\n",
        "            final_df = pd.concat([final_df, sampled_data])\n",
        "        else:\n",
        "            final_df = pd.concat([final_df, cluster_data])\n",
        "\n",
        "final_df.drop(columns = ['Cluster'], inplace = True)\n",
        "final_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8HLjbPpkX1E",
        "outputId": "7aeee4fa-f401-42ab-ae96-f8aac1b685bd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-dabe7913eada>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  subset['Cluster'] = labels\n",
            "<ipython-input-30-dabe7913eada>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  subset['Cluster'] = labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 9544 entries, 0 to 24617\n",
            "Data columns (total 3 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Trích yếu       9544 non-null   object\n",
            " 1   ID phòng xử lý  9544 non-null   int64 \n",
            " 2   input           9544 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 298.2+ KB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-dabe7913eada>:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  subset['Cluster'] = labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = final_df\n",
        "df['ID phòng xử lý'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQXiWQqHqVrk",
        "outputId": "203b38bb-1b6c-4f88-a5b4-eaeb6027c348"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ID phòng xử lý\n",
              "2733540    1997\n",
              "2733546    1997\n",
              "2733552    1997\n",
              "2733570    1593\n",
              "2733522    1345\n",
              "2733534     615\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_output = '/content/gdrive/MyDrive/Thesis/dataset/PreprocessedData'\n",
        "name_file_output = 'data_2_nam_keep_multi_label_and_remove_np_and_kmean_2000.csv'\n",
        "import os\n",
        "if not os.path.exists(path_output):\n",
        "    os.makedirs(path_output)\n",
        "\n",
        "# Ghi DataFrame all_data vào file CSV\n",
        "df.to_csv(os.path.join(path_output, name_file_output), index=False)"
      ],
      "metadata": {
        "id": "_9KthzHDD7J8"
      },
      "execution_count": 32,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}